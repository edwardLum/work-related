{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOmOfEtwadf+byxSPQkLeT6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardLum/work-related/blob/main/clustering_search_terms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "4Ic-6eIYW_yL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**\n",
        "\n",
        "Libraries used:\n",
        "\n",
        "* **pandas**: Pandas pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language. User guide [here](https://pandas.pydata.org/docs/user_guide/index.**html**)\n",
        "\n",
        "* **gensim**: Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Documentation [here](https://github.com/RaRe-Technologies/gensim/#documentation)\n",
        "\n",
        "* **sklean**: scikit-learn is a free software machine learning library for the Python programming language.[3] It features various classification, regression and clustering algorithms and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. User guide [here](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "uNwErbmYZ_fE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jZLbr63vcfn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Files upload**\n",
        "\n",
        "Choose file to upload and pass them to a list"
      ],
      "metadata": {
        "id": "pMrr23XvNAHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "filenames = []\n",
        "\n",
        "# Upload files:\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  filenames.append(fn)"
      ],
      "metadata": {
        "id": "w8xmPTKOM0oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect encoding**\n",
        "\n",
        "Use the detect method of chardet to detect the encoding of the provided file."
      ],
      "metadata": {
        "id": "0Qp5XuqfdJfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        result = chardet.detect(f.read())\n",
        "    return result['encoding']\n",
        "\n",
        "file_path = f\"/content/{filenames[0]}\"\n",
        "original_encoding = detect_encoding(file_path)\n",
        "\n",
        "print(f\"Detected encoding: {original_encoding}\")"
      ],
      "metadata": {
        "id": "flNdRgOXwynw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation"
      ],
      "metadata": {
        "id": "hTwf99hjW2x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**\n",
        "\n",
        "Create dataframe using the provided csv. Provide:\n",
        "\n",
        "* the separator the csv uses\n",
        "* the encoding of the file\n",
        "* how many rows have to be skipped (if any)\n",
        "* thousands separator\n",
        "\n",
        "Remove summary rows if any."
      ],
      "metadata": {
        "id": "wWD0fbHcd875"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "all_terms = pd.read_csv(file_path, sep='\\t',\n",
        "                        encoding=original_encoding,\n",
        "                        skiprows=2,\n",
        "                        thousands=',')"
      ],
      "metadata": {
        "id": "_dUA0gcNvlJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process data**\n",
        "\n",
        "Actions:\n",
        "* Remove summary rows\n",
        "* Remove unnecessary colums\n",
        "* Deduplicate and aggregate metrics\n"
      ],
      "metadata": {
        "id": "TdX5DUn8X5MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Summary rows\n",
        "all_terms = all_terms[~all_terms['Search term'].str.startswith('Total: ')]\n",
        "\n",
        "# Remove unnecessary columns\n",
        "columns_to_drop = ['Conv. rate', 'CTR', 'Cost / conv.', 'Avg. CPC']\n",
        "all_terms_required_columns = all_terms.drop(columns=columns_to_drop)\n",
        "\n",
        "aggregated_terms = all_terms_required_columns.groupby('Search term').agg({\n",
        "    'Clicks': 'sum',\n",
        "    'Cost': 'sum',\n",
        "    'Impr.': 'sum',\n",
        "    'Conversions': 'sum',\n",
        "}).reset_index()\n",
        "\n",
        "data = aggregated_terms"
      ],
      "metadata": {
        "id": "2oPke-xHYIHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "hYVzDfKyW7fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF vectorization**\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) vectorization is a technique used to quantify the importance of words in a document relative to a corpus. It weighs terms based on their frequency in a document, penalized by their frequency across all documents. This results in emphasizing terms that are unique or specific to a particular document, while downplaying common or repetitive terms.\n",
        "\n",
        "Need to provide two parameters:\n",
        "\n",
        "**max_df**: This parameter is used to remove terms that appear too frequently in the corpus. It can be either:\n",
        "an integer (e.g., 5), which specifies the maximum number of documents a term can appear in for it to be included as a feature, or\n",
        "a float (e.g., 0.85), which represents a proportion of the entire corpus.\n",
        "\n",
        "If a term appears in more than this proportion of documents, it will be discarded.\n",
        "The main idea behind max_df is that words appearing in a very large proportion of documents are likely to be common words (e.g., stopwords) that might not carry specific, meaningful information about the content of a document.\n",
        "\n",
        "**max_features**: This parameter limits the number of top features (words or tokens) the vectorizer will learn from the corpus based on term frequency.\n",
        "If set (e.g., to 10000), the vectorizer will only consider the top max_features ordered by term frequency across the corpus.\n",
        "\n",
        "This can be useful to limit the dimensionality of the output, especially when dealing with very large datasets where memory or computational resources are a concern."
      ],
      "metadata": {
        "id": "FJ2D2Iklgrtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=10000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Search term'])"
      ],
      "metadata": {
        "id": "-5j0hbcsnLDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect TF-IDF output**\n",
        "\n",
        "**Print feature names**: which words or tokens the vectorizer has recognized in given corpus"
      ],
      "metadata": {
        "id": "VQw3_1xVR-jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "b0dffrynSZt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check IDF values**\n",
        "\n",
        "This provides an idea of how unique or rare each word is across the entire corpus.\n",
        "\n",
        "Higher IDF values mean the word is rarer across your documents."
      ],
      "metadata": {
        "id": "5bxvq1_KWP9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idf_values = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
        "idf_df = pd.DataFrame(list(idf_values.items()), columns=[\"token\", \"idf\"]).sort_values(by=\"idf\", ascending=False)\n",
        "\n",
        "print(idf_df)"
      ],
      "metadata": {
        "id": "isWPClNJS-CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review Max and Min TF-IDF Scores**\n",
        "\n",
        "Understand the range of values and potentially identify terms that are too common or too rare."
      ],
      "metadata": {
        "id": "svQ4VH5VXPpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tf_idf = tfidf_matrix.max(axis=0).toarray().ravel()\n",
        "sorted_by_tfidf = max_tf_idf.argsort()\n",
        "\n",
        "print(\"Tokens with highest tfidf:\", [feature_names[i] for i in sorted_by_tfidf[-10:]])\n",
        "print(\"Tokens with lowest tfidf:\", [feature_names[i] for i in sorted_by_tfidf[:10]])"
      ],
      "metadata": {
        "id": "0IWdLzNqXZJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "CeERHbF_YJ0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of clusters - You might want to adjust this based on domain knowledge\n",
        "n_clusters = 70\n",
        "\n",
        "# Clustering for TF-IDF\n",
        "kmeans_tfidf = KMeans(n_clusters=n_clusters)\n",
        "data['tfidf_cluster'] = kmeans_tfidf.fit_predict(tfidf_matrix)"
      ],
      "metadata": {
        "id": "hvX4BOXmYEwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading output CSV**"
      ],
      "metadata": {
        "id": "qpoub1w5YnXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"terms-with-clusters.csv\"\n",
        "data.to_csv(filename,\n",
        "            index=False,\n",
        "            thousands=',')\n",
        "\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "FoK56uyRYmD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(data[data['tfidf_cluster'] == i]['Search term'])\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "iayMRkckZIEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming tfidf_matrix from your previous code is your data\n",
        "range_n_clusters = [40, 50, 60, 70]  # you can extend this as needed\n",
        "X = tfidf_matrix.toarray()\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = plt.cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    plt.suptitle(f\"Silhouette analysis for KMeans clustering with n_clusters = {n_clusters}\", fontsize=14, fontweight='bold')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "vcLF5kvocZu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}