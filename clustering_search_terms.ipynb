{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnjPluqiOL2cBZi4p/9j8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwardLum/work-related/blob/main/clustering_search_terms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "4Ic-6eIYW_yL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**\n",
        "\n",
        "Libraries used:\n",
        "\n",
        "* **pandas**: Pandas pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
        "built on top of the Python programming language. User guide [here](https://pandas.pydata.org/docs/user_guide/index.**html**)\n",
        "\n",
        "* **gensim**: Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Documentation [here](https://github.com/RaRe-Technologies/gensim/#documentation)\n",
        "\n",
        "* **sklean**: scikit-learn is a free software machine learning library for the Python programming language.[3] It features various classification, regression and clustering algorithms and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. User guide [here](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "uNwErbmYZ_fE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-jZLbr63vcfn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Files upload**\n",
        "\n",
        "Choose file to upload and pass them to a list"
      ],
      "metadata": {
        "id": "pMrr23XvNAHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "filenames = []\n",
        "\n",
        "# Upload files:\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  filenames.append(fn)"
      ],
      "metadata": {
        "id": "w8xmPTKOM0oW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e58dbc6b-d9f3-4758-a5b6-723f2498a39d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a9021d2-c2b5-4cb9-9b80-ce2c0ea348aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5a9021d2-c2b5-4cb9-9b80-ce2c0ea348aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Search terms report - 2023-10-17T184921.753.csv to Search terms report - 2023-10-17T184921.753 (1).csv\n",
            "User uploaded file \"Search terms report - 2023-10-17T184921.753 (1).csv\" with length 222554 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect encoding**\n",
        "\n",
        "Use the detect method of chardet to detect the encoding of the provided file."
      ],
      "metadata": {
        "id": "0Qp5XuqfdJfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_encoding(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        result = chardet.detect(f.read())\n",
        "    return result['encoding']\n",
        "\n",
        "file_path = f\"/content/{filenames[0]}\"\n",
        "original_encoding = detect_encoding(file_path)\n",
        "\n",
        "print(f\"Detected encoding: {original_encoding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNdRgOXwynw",
        "outputId": "6b410789-469c-44d2-dd38-f1eaa74a40d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected encoding: UTF-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation"
      ],
      "metadata": {
        "id": "hTwf99hjW2x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read data**\n",
        "\n",
        "Create dataframe using the provided csv. Provide:\n",
        "\n",
        "* the separator the csv uses\n",
        "* the encoding of the file\n",
        "* how many rows have to be skipped (if any)\n",
        "* thousands separator\n",
        "\n",
        "Remove summary rows if any."
      ],
      "metadata": {
        "id": "wWD0fbHcd875"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "all_terms = pd.read_csv(file_path, sep='\\t',\n",
        "                        encoding=original_encoding,\n",
        "                        skiprows=2,\n",
        "                        thousands=',')"
      ],
      "metadata": {
        "id": "_dUA0gcNvlJm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process data**\n",
        "\n",
        "Actions:\n",
        "* Remove summary rows\n",
        "* Remove unnecessary colums\n",
        "* Deduplicate and aggregate metrics\n"
      ],
      "metadata": {
        "id": "TdX5DUn8X5MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Summary rows\n",
        "all_terms = all_terms[~all_terms['Search term'].str.startswith('Total: ')]\n",
        "\n",
        "# Remove unnecessary columns\n",
        "columns_to_drop = ['Conv. rate', 'CTR', 'Cost / conv.', 'Avg. CPC']\n",
        "all_terms_required_columns = all_terms.drop(columns=columns_to_drop)\n",
        "\n",
        "aggregated_terms = all_terms_required_columns.groupby('Search term').agg({\n",
        "    'Clicks': 'sum',\n",
        "    'Cost': 'sum',\n",
        "    'Impr.': 'sum',\n",
        "    'Conversions': 'sum',\n",
        "}).reset_index()\n",
        "\n",
        "data = aggregated_terms"
      ],
      "metadata": {
        "id": "2oPke-xHYIHv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "hYVzDfKyW7fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF vectorization**\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) vectorization is a technique used to quantify the importance of words in a document relative to a corpus. It weighs terms based on their frequency in a document, penalized by their frequency across all documents. This results in emphasizing terms that are unique or specific to a particular document, while downplaying common or repetitive terms.\n",
        "\n",
        "Need to provide two parameters:\n",
        "\n",
        "**max_df**: This parameter is used to remove terms that appear too frequently in the corpus. It can be either:\n",
        "an integer (e.g., 5), which specifies the maximum number of documents a term can appear in for it to be included as a feature, or\n",
        "a float (e.g., 0.85), which represents a proportion of the entire corpus.\n",
        "\n",
        "If a term appears in more than this proportion of documents, it will be discarded.\n",
        "The main idea behind max_df is that words appearing in a very large proportion of documents are likely to be common words (e.g., stopwords) that might not carry specific, meaningful information about the content of a document.\n",
        "\n",
        "**max_features**: This parameter limits the number of top features (words or tokens) the vectorizer will learn from the corpus based on term frequency.\n",
        "If set (e.g., to 10000), the vectorizer will only consider the top max_features ordered by term frequency across the corpus.\n",
        "\n",
        "This can be useful to limit the dimensionality of the output, especially when dealing with very large datasets where memory or computational resources are a concern."
      ],
      "metadata": {
        "id": "FJ2D2Iklgrtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, max_features=10000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Search term'])"
      ],
      "metadata": {
        "id": "-5j0hbcsnLDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inspect TF-IDF output**\n",
        "\n",
        "**Print feature names**: which words or tokens the vectorizer has recognized in given corpus"
      ],
      "metadata": {
        "id": "VQw3_1xVR-jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0dffrynSZt7",
        "outputId": "17c017b3-acf9-4737-c594-f07d0cf9eeeb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10' '100' '10bet' '1bet' '1x' '1xbet' '22bet' '365' '4rabet' '50'\n",
            " '888sport' 'account' 'activate' 'adsrv' 'africa' 'african' 'airtime'\n",
            " 'alp' 'apk' 'app' 'apps' 'aviator' 'batway' 'best' 'bestsportsbetting'\n",
            " 'bet' 'bet365' 'betbright' 'betfair' 'betfred' 'betking' 'betnow'\n",
            " 'betolimp' 'betonline' 'betpawa' 'bets' 'bettabets' 'better' 'betting'\n",
            " 'betup' 'betway' 'betxchange' 'bonus' 'buy' 'cash' 'casino' 'change'\n",
            " 'com' 'companies' 'contact' 'correct' 'cup' 'day' 'deposit' 'details'\n",
            " 'does' 'download' 'eacdn' 'easy' 'ebet' 'esports' 'exchange' 'firstbet'\n",
            " 'football' 'fredbet' 'free' 'gambling' 'game' 'games' 'gbet' 'gbets'\n",
            " 'gives' 'hack' 'hollywood' 'hollywoodbet' 'hollywoodbets' 'interbet'\n",
            " 'international' 'jabula' 'jackpot' 'land' 'live' 'log' 'login' 'lotto'\n",
            " 'lottoland' 'lottostar' 'mbet' 'mega' 'mines' 'mobile' 'money' 'net'\n",
            " 'new' 'news' 'number' 'odds' 'online' 'ontario' 'ott' 'palace'\n",
            " 'palacebet' 'palmerbet' 'paripesa' 'pesa' 'picks' 'pin' 'place' 'play'\n",
            " 'playa' 'playbet' 'poker' 'pokerbet' 'pool' 'popular' 'powerbet'\n",
            " 'powerbets' 'r25' 'real' 'register' 'registering' 'registration'\n",
            " 'reliable' 'sa' 'score' 'shezi' 'showing' 'sign' 'site' 'sites' 'sky'\n",
            " 'slots' 'soccer' 'soccerbet' 'soccerbets' 'soccershop' 'south' 'spin'\n",
            " 'spina' 'spins' 'sport' 'sportbet' 'sporting' 'sportingbet' 'sportpesa'\n",
            " 'sports' 'sportsbet' 'sportsbetting' 'sportspersa' 'sportspesa'\n",
            " 'sportspessa' 'sportybet' 'spot' 'star' 'sun' 'sunbet' 'sunbets' 'supa'\n",
            " 'supabet' 'supabets' 'super' 'superbet' 'superbets' 'superpicks'\n",
            " 'supersport' 'supper' 'swb' 'tab' 'tictac' 'tips' 'topbet' 'type' 'ubet'\n",
            " 'uk' 'unibet' 'unlock' 'using' 'valid' 'vegas' 'voucher' 'vs' 'websites'\n",
            " 'welcome' 'win' 'wlhollywoodbets' 'world' 'worldbet' 'worldsportbet'\n",
            " 'worldsports' 'worldsportsbetting' 'ws' 'wsb' 'www' 'xbet' 'xbets'\n",
            " 'yankee' 'yesplay' 'za' 'zonke' 'zw']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check IDF values**\n",
        "\n",
        "This provides an idea of how unique or rare each word is across the entire corpus.\n",
        "\n",
        "Higher IDF values mean the word is rarer across your documents."
      ],
      "metadata": {
        "id": "5bxvq1_KWP9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idf_values = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
        "idf_df = pd.DataFrame(list(idf_values.items()), columns=[\"token\", \"idf\"]).sort_values(by=\"idf\", ascending=False)\n",
        "\n",
        "print(idf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isWPClNJS-CS",
        "outputId": "46bfb827-7bdc-4ad9-b07c-b2e693d45761"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          token       idf\n",
            "0            10  6.365976\n",
            "134  soccerbets  6.365976\n",
            "108        play  6.365976\n",
            "109       playa  6.365976\n",
            "111       poker  6.365976\n",
            "..          ...       ...\n",
            "119    register  3.274934\n",
            "40       betway  3.208976\n",
            "25          bet  2.782457\n",
            "83        login  2.755058\n",
            "38      betting  2.424394\n",
            "\n",
            "[200 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review Max and Min TF-IDF Scores**\n",
        "\n",
        "Understand the range of values and potentially identify terms that are too common or too rare."
      ],
      "metadata": {
        "id": "svQ4VH5VXPpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tf_idf = tfidf_matrix.max(axis=0).toarray().ravel()\n",
        "sorted_by_tfidf = max_tf_idf.argsort()\n",
        "\n",
        "print(\"Tokens with highest tfidf:\", [feature_names[i] for i in sorted_by_tfidf[-10:]])\n",
        "print(\"Tokens with lowest tfidf:\", [feature_names[i] for i in sorted_by_tfidf[:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IWdLzNqXZJk",
        "outputId": "2c650ced-fa67-4401-bd5c-5ef7a7c0015f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens with highest tfidf: ['sunbet', 'sunbets', 'betway', 'superbet', 'bet365', 'bet', 'betonline', 'gbet', 'sportybet', 'sportsbetting']\n",
            "Tokens with lowest tfidf: ['spin', 'real', 'using', 'buy', '50', 'adsrv', 'wlhollywoodbets', 'eacdn', 'win', 'south']\n"
          ]
        }
      ]
    }
  ]
}